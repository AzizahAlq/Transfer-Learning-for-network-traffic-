{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOZtreDX42ETA0pjAVH7UsN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# the new dataset from week of April\n","\n","Key Components:\n","Imports: All necessary libraries are imported.\n","File Paths: File paths for the scaler, autoencoder, and threshold are defined.\n","Resource Loading: Resources are loaded from the specified paths.\n","Utility Functions:\n","ip_to_int: Converts IP addresses to integers.\n","preprocess_samples: Preprocesses the DataFrame by converting IPs, encoding categorical features, imputing missing values, and scaling.\n","predict_anomalies: Applies the preprocessing function, predicts reconstruction errors, and identifies anomalies.\n","Dataset Processing:\n","Loads a larger dataset from a CSV file.\n","Processes the dataset to detect anomalies.\n","Prints the results, including reconstruction errors and anomaly labels.\n","This script assumes that you have a CSV file containing the dataset with the required features and that your model and scaler were trained with a similar schema. Adjust file paths, mappings, and feature names as needed based on your specific context."],"metadata":{"id":"uc8n0SQbqwTc"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import struct\n","import socket\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from tensorflow.keras.models import load_model\n","from joblib import dump, load\n","\n","# Define file paths\n","scaler_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/scaler.joblib'\n","autoencoder_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/autoencoder_model.keras'\n","threshold_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/threshold.npy'\n","\n","# Load resources\n","scaler = load(scaler_file_path)\n","autoencoder = load_model(autoencoder_file_path)\n","threshold = np.load(threshold_file_path)\n","\n","# Load a larger dataset\n","dataset_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/new_dataset.csv'\n","new_dataset_df = pd.read_csv(dataset_file_path)\n","\n","# Print columns to check for discrepancies\n","print(\"Columns in the dataset:\", new_dataset_df.columns)\n","\n","# Categorical features to be encoded\n","categorical_features = ['protocol', 'flag']\n","\n","# Initialize mappings dictionary\n","mappings = {}\n","\n","# Perform label encoding for categorical features and store mappings\n","for feature in categorical_features:\n","    # Create a LabelEncoder object\n","    label_encoder = LabelEncoder()\n","    # Fit and transform the feature in new_dataset_df\n","    new_dataset_df[feature] = label_encoder.fit_transform(new_dataset_df[feature])\n","    # Store the mapping for later use\n","    mappings[feature] = {label: index for index, label in enumerate(label_encoder.classes_)}\n","\n","# Extract mappings for protocol and flag\n","protocol_mapping = mappings.get('protocol', {})\n","flag_mapping = mappings.get('flag', {})\n","\n","def ip_to_int(ip_address):\n","    try:\n","        return struct.unpack(\"!I\", socket.inet_aton(ip_address))[0]\n","    except socket.error:\n","        return 0  # Return 0 if IP address is invalid\n","\n","def preprocess_samples(samples_df, scaler):\n","    # Ensure all required features are present and impute missing values with zeros\n","    original_features = ['duration', 'source_ip', 'destination_ip', 'source_port',\n","                         'destination_port', 'forwarding_status', 'protocol',\n","                         'flag', 'tos', 'packets', 'bytes']\n","\n","    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n","    samples_df = samples_df.copy()\n","\n","    for feature in original_features:\n","        if feature not in samples_df:\n","            samples_df[feature] = 0  # Impute missing features with zeros\n","\n","    # Convert IP addresses to integers\n","    samples_df['source_ip'] = samples_df['source_ip'].apply(ip_to_int)\n","    samples_df['destination_ip'] = samples_df['destination_ip'].apply(ip_to_int)\n","\n","    # Encode categorical features\n","    samples_df['protocol'] = samples_df['protocol'].map(protocol_mapping).fillna(-1).astype(int)\n","    samples_df['flag'] = samples_df['flag'].map(flag_mapping).fillna(-1).astype(int)\n","\n","    # Ensure that the DataFrame only contains columns that the scaler expects\n","    if set(original_features) - set(samples_df.columns):\n","        raise ValueError(\"Missing features in the DataFrame.\")\n","\n","    # Reorder columns to match the scaler's expected feature order\n","    samples_df = samples_df[original_features]\n","\n","    # Fill missing values in numerical features\n","    samples_df.fillna(0, inplace=True)\n","\n","\n","    # Normalize the data using StandardScaler\n","    scaler = StandardScaler()\n","    samples_normalized = scaler.fit_transform(samples_df)\n","\n","    return samples_df\n","\n","\n","\n","def predict_anomalies(samples_df, scaler, autoencoder, threshold):\n","    # Preprocess the samples\n","    preprocessed_samples_df = preprocess_samples(samples_df, scaler)\n","\n","    # Predict reconstructed data using the autoencoder\n","    reconstructed_data = autoencoder.predict(preprocessed_samples_df)\n","\n","    # Calculate reconstruction errors\n","    reconstruction_errors = np.mean(np.square(preprocessed_samples_df - reconstructed_data), axis=1)\n","\n","    # Determine if each reconstruction error exceeds the threshold\n","    anomalies = reconstruction_errors > threshold\n","\n","    # Map the anomalies to the expected output\n","    results_df = samples_df.copy()\n","    results_df['reconstruction_error'] = reconstruction_errors\n","    results_df['anomaly'] = ['Background' if not is_anomaly else 'Anomaly' for is_anomaly in anomalies]\n","\n","    return results_df\n","\n","def preprocess_new_sample(new_sample, scaler, protocol_mapping, flag_mapping):\n","    # Create a DataFrame for the new sample\n","    new_sample_df = pd.DataFrame([new_sample])\n","\n","    # Preprocess the new sample\n","    return preprocess_samples(new_sample_df, scaler)\n","\n","def predict_anomaly(new_sample, scaler, autoencoder, threshold):\n","    # Preprocess the new sample\n","    new_sample_df = preprocess_new_sample(new_sample, scaler, protocol_mapping, flag_mapping)\n","\n","    # Predict reconstructed data using the autoencoder\n","    reconstructed_data = autoencoder.predict(new_sample_df)\n","\n","    # Calculate reconstruction error\n","    reconstruction_error = np.mean(np.square(new_sample_df.values - reconstructed_data))\n","\n","    # Determine if the reconstruction error exceeds the threshold\n","    is_anomaly = reconstruction_error > threshold\n","\n","    # Return the anomaly status and reconstruction error\n","    return \"Anomaly\" if is_anomaly else \"Normal\", reconstruction_error\n","\n","# Call the function and capture the results\n","results_df = predict_anomalies(new_dataset_df, scaler, autoencoder, threshold)\n","\n","# Print results\n","print(results_df.head())\n"],"metadata":{"id":"hOQYgeTyzPb7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720578975700,"user_tz":-180,"elapsed":6117,"user":{"displayName":"Azizah alqahtani","userId":"11981846926504147280"}},"outputId":"6eb06be0-3d14-4121-8a44-b7e262a438c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Columns in the dataset: Index(['date_time', 'duration', 'source_ip', 'destination_ip', 'source_port',\n","       'destination_port', 'protocol', 'flag', 'forwarding_status', 'tos',\n","       'packets', 'bytes', 'label'],\n","      dtype='object')\n","968/968 [==============================] - 3s 3ms/step\n","          date_time  duration       source_ip  destination_ip  source_port  \\\n","0  27/07/2016 18:29     0.668    211.58.234.3   42.219.155.56        50099   \n","1   28/07/2016 4:29     0.540    193.27.1.120  42.219.156.212           25   \n","2  27/07/2016 17:17     3.604  70.210.176.159   42.219.155.28        33990   \n","3   28/07/2016 0:49     0.000   42.219.156.30  42.219.150.247         7921   \n","4   28/07/2016 3:52     0.000   43.164.49.177   42.219.155.26           80   \n","\n","   destination_port  protocol  flag  forwarding_status  tos  packets  bytes  \\\n","0                80         5    15                  0    0       34   1988   \n","1             40937         5    15                  0    0        5    961   \n","2               443         5    19                  0    0       14   2953   \n","3             63479         5     8                  0    0        1     40   \n","4             56831         5     5                  0    0        1     52   \n","\n","          label  reconstruction_error  anomaly  \n","0    background          1.188719e+18  Anomaly  \n","1  anomaly-spam          1.001194e+18  Anomaly  \n","2    background          1.753510e+17  Anomaly  \n","3        scan44          9.400200e+16  Anomaly  \n","4    background          9.573641e+16  Anomaly  \n"]}]},{"cell_type":"markdown","source":["1- Load Pre-trained Model: Load a pre-trained model that has been trained on a similar dataset.\n","2- Adjust Architecture: Modify or add layers as needed for the new task.\n","3- Retrain: Fine-tune the model on the new dataset by continuing the training process.\n","4- Evaluate: Assess the performance of the fine-tuned model using metrics like classification report, confusion matrix, and ROC curve."],"metadata":{"id":"Eh1elNtgpGCy"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import struct\n","import socket\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from tensorflow.keras.models import load_model\n","from joblib import load\n","\n","# Define file paths\n","scaler_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/scaler.joblib'\n","autoencoder_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/autoencoder_model.keras'\n","threshold_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/threshold.npy'\n","\n","# Load resources\n","scaler = load(scaler_file_path)  # Load the pre-trained scaler\n","autoencoder = load_model(autoencoder_file_path)\n","threshold = np.load(threshold_file_path)\n","\n","# Load a larger dataset\n","dataset_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/new_dataset.csv'\n","new_dataset_df = pd.read_csv(dataset_file_path)\n","\n","# Print columns to check for discrepancies\n","print(\"Columns in the dataset:\", new_dataset_df.columns)\n","\n","# Categorical features to be encoded\n","categorical_features = ['protocol', 'flag']\n","\n","# Initialize mappings dictionary\n","mappings = {}\n","\n","# Perform label encoding for categorical features and store mappings\n","for feature in categorical_features:\n","    # Create a LabelEncoder object\n","    label_encoder = LabelEncoder()\n","    # Fit and transform the feature in new_dataset_df\n","    new_dataset_df[feature] = label_encoder.fit_transform(new_dataset_df[feature])\n","    # Store the mapping for later use\n","    mappings[feature] = {label: index for index, label in enumerate(label_encoder.classes_)}\n","\n","# Extract mappings for protocol and flag\n","protocol_mapping = mappings.get('protocol', {})\n","flag_mapping = mappings.get('flag', {})\n","\n","def ip_to_int(ip_address):\n","    try:\n","        return struct.unpack(\"!I\", socket.inet_aton(ip_address))[0]\n","    except socket.error:\n","        return 0  # Return 0 if IP address is invalid\n","\n","def preprocess_samples(samples_df, scaler):\n","    # Define the exact column order as used during scaler fitting\n","    original_features = ['duration', 'source_ip', 'destination_ip', 'source_port',\n","                         'destination_port', 'forwarding_status', 'protocol',\n","                         'flag', 'tos', 'packets', 'bytes']\n","\n","    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n","    samples_df = samples_df.copy()\n","\n","    # Ensure all required features are present and impute missing values with zeros\n","    for feature in original_features:\n","        if feature not in samples_df:\n","            samples_df[feature] = 0  # Impute missing features with zeros\n","\n","    # Convert IP addresses to integers\n","    samples_df['source_ip'] = samples_df['source_ip'].apply(ip_to_int)\n","    samples_df['destination_ip'] = samples_df['destination_ip'].apply(ip_to_int)\n","\n","    # Encode categorical features\n","    samples_df['protocol'] = samples_df['protocol'].map(protocol_mapping).fillna(-1).astype(int)\n","    samples_df['flag'] = samples_df['flag'].map(flag_mapping).fillna(-1).astype(int)\n","\n","    # Reorder columns to match the scaler's expected feature order\n","    samples_df = samples_df[original_features]\n","\n","    # Fill missing values in numerical features\n","    samples_df.fillna(0, inplace=True)\n","\n","    # Print column names before scaling\n","    print(\"Columns before scaling:\", samples_df.columns)\n","\n","    # Normalize the data using the pre-loaded StandardScaler\n","    samples_normalized = scaler.transform(samples_df)\n","\n","    # Print column names after scaling\n","    print(\"Columns after scaling:\", samples_df.columns)\n","\n","    return samples_normalized\n","\n","def predict_anomalies(samples_df, scaler, autoencoder, threshold):\n","    # Preprocess the samples\n","    preprocessed_samples_df = preprocess_samples(samples_df, scaler)\n","\n","    # Predict reconstructed data using the autoencoder\n","    reconstructed_data = autoencoder.predict(preprocessed_samples_df)\n","\n","    # Calculate reconstruction errors\n","    reconstruction_errors = np.mean(np.square(preprocessed_samples_df - reconstructed_data), axis=1)\n","\n","    # Determine if each reconstruction error exceeds the threshold\n","    anomalies = reconstruction_errors > threshold\n","\n","    # Map the anomalies to the expected output\n","    results_df = samples_df.copy()\n","    results_df['reconstruction_error'] = reconstruction_errors\n","    results_df['anomaly'] = ['Background' if not is_anomaly else 'Anomaly' for is_anomaly in anomalies]\n","\n","    return results_df\n","\n","def preprocess_new_sample(new_sample, scaler, protocol_mapping, flag_mapping):\n","    # Create a DataFrame for the new sample\n","    new_sample_df = pd.DataFrame([new_sample])\n","\n","    # Preprocess the new sample\n","    return preprocess_samples(new_sample_df, scaler)\n","\n","# Call the function and capture the results\n","results_df = predict_anomalies(new_dataset_df, scaler, autoencoder, threshold)\n","\n","# Print results\n","print(results_df.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":512},"id":"mdoM1P9agKdM","executionInfo":{"status":"error","timestamp":1720579915141,"user_tz":-180,"elapsed":894,"user":{"displayName":"Azizah alqahtani","userId":"11981846926504147280"}},"outputId":"e8801dbf-f222-4afe-e3b9-103622425f5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Columns in the dataset: Index(['date_time', 'duration', 'source_ip', 'destination_ip', 'source_port',\n","       'destination_port', 'protocol', 'flag', 'forwarding_status', 'tos',\n","       'packets', 'bytes', 'label'],\n","      dtype='object')\n","Columns before scaling: Index(['duration', 'source_ip', 'destination_ip', 'source_port',\n","       'destination_port', 'forwarding_status', 'protocol', 'flag', 'tos',\n","       'packets', 'bytes'],\n","      dtype='object')\n"]},{"output_type":"error","ename":"ValueError","evalue":"The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-100-bc7fca5b580e>\u001b[0m in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;31m# Call the function and capture the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_anomalies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dataset_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# Print results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-100-bc7fca5b580e>\u001b[0m in \u001b[0;36mpredict_anomalies\u001b[0;34m(samples_df, scaler, autoencoder, threshold)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_anomalies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Preprocess the samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mpreprocessed_samples_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Predict reconstructed data using the autoencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-100-bc7fca5b580e>\u001b[0m in \u001b[0;36mpreprocess_samples\u001b[0;34m(samples_df, scaler)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# Normalize the data using the pre-loaded StandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0msamples_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# Print column names after scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    993\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mvalidated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 )\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     def _validate_data(\n","\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n"]}]},{"cell_type":"code","source":["# Save the entire DataFrame to a CSV file\n","results_df.to_csv('/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/results_df.csv', index=False)\n","print(\"Results saved to /content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/results_df.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWWUwix_L0iG","executionInfo":{"status":"ok","timestamp":1720578760925,"user_tz":-180,"elapsed":414,"user":{"displayName":"Azizah alqahtani","userId":"11981846926504147280"}},"outputId":"ee546d31-3c3d-4cb4-8b44-ff3f75a6aaba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Results saved to /content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/results_df.csv\n"]}]},{"cell_type":"code","source":["# Print the entire DataFrame (use cautiously for very large datasets)\n","with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n","    print(results_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1QYspiw83ERXxn08w5yOFh4ktjL_NdHl2"},"id":"L-uWbRrlL2_6","executionInfo":{"status":"ok","timestamp":1720578772834,"user_tz":-180,"elapsed":8706,"user":{"displayName":"Azizah alqahtani","userId":"11981846926504147280"}},"outputId":"b3f0f3ee-4844-4b7b-edc7-6905dd370af2"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# 1. Loading the Pre-trained Model\n"],"metadata":{"id":"yv4cNrg9mp5y"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import struct\n","import socket\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tensorflow.keras.models import load_model\n","from joblib import load\n","\n","# Define file paths\n","scaler_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/scaler.joblib'\n","autoencoder_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/autoencoder_model.keras'\n","threshold_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/threshold.npy'\n","dataset_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/new_dataset.csv'\n","\n","# Load resources\n","scaler = load(scaler_file_path)\n","autoencoder = load_model(autoencoder_file_path)\n","threshold = np.load(threshold_file_path)\n","\n","# Load the dataset\n","dataset_df = pd.read_csv(dataset_file_path)\n","\n","# Print columns to check for discrepancies\n","print(\"Columns in the dataset:\", dataset_df.columns)\n","\n","# Categorical features to be encoded\n","categorical_features = ['protocol', 'flag']\n","\n","# Initialize mappings dictionary\n","mappings = {}\n","\n","# Perform label encoding for categorical features and store mappings\n","for feature in categorical_features:\n","    label_encoder = LabelEncoder()\n","    dataset_df[feature] = label_encoder.fit_transform(dataset_df[feature])\n","    mappings[feature] = {label: index for index, label in enumerate(label_encoder.classes_)}\n","\n","# Extract mappings for protocol and flag\n","protocol_mapping = mappings.get('protocol', {})\n","flag_mapping = mappings.get('flag', {})\n","\n","def ip_to_int(ip_address):\n","    try:\n","        return struct.unpack(\"!I\", socket.inet_aton(ip_address))[0]\n","    except socket.error:\n","        return 0  # Return 0 if IP address is invalid\n","\n","def preprocess_samples(samples_df, scaler):\n","    # Ensure all required features are present and impute missing values with zeros\n","    original_features = ['duration', 'source_ip', 'destination_ip', 'source_port',\n","                         'destination_port', 'forwarding_status', 'protocol',\n","                         'flag', 'tos', 'packets', 'bytes', 'label']\n","\n","    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n","    samples_df = samples_df.copy()\n","\n","    for feature in original_features:\n","        if feature not in samples_df:\n","            samples_df[feature] = 0  # Impute missing features with zeros\n","\n","    # Convert IP addresses to integers\n","    samples_df['source_ip'] = samples_df['source_ip'].apply(ip_to_int)\n","    samples_df['destination_ip'] = samples_df['destination_ip'].apply(ip_to_int)\n","\n","    # Encode categorical features\n","    samples_df['protocol'] = samples_df['protocol'].map(protocol_mapping).fillna(-1).astype(int)\n","    samples_df['flag'] = samples_df['flag'].map(flag_mapping).fillna(-1).astype(int)\n","\n","    # Ensure that the DataFrame only contains columns that the scaler expects\n","    if set(original_features) - set(samples_df.columns):\n","        raise ValueError(\"Missing features in the DataFrame.\")\n","\n","    # Reorder columns to match the scaler's expected feature order\n","    samples_df = samples_df[original_features]\n","\n","    # Fill missing values in numerical features\n","    samples_df.fillna(0, inplace=True)\n","\n","    # Normalize the data using StandardScaler\n","    samples_normalized = scaler.transform(samples_df)  # Use the pre-fitted scaler\n","\n","    return samples_normalized\n","\n","def predict_anomalies(samples_df, scaler, autoencoder, threshold):\n","    # Preprocess the samples\n","    preprocessed_samples_df = preprocess_samples(samples_df, scaler)\n","\n","    # Predict reconstructed data using the autoencoder\n","    reconstructed_data = autoencoder.predict(preprocessed_samples_df)\n","\n","    # Calculate reconstruction errors\n","    reconstruction_errors = np.mean(np.square(preprocessed_samples_df - reconstructed_data), axis=1)\n","\n","    # Determine if each reconstruction error exceeds the threshold\n","    anomalies = reconstruction_errors > threshold\n","\n","    # Return a DataFrame with the results\n","    results_df = samples_df.copy()\n","    results_df['reconstruction_error'] = reconstruction_errors\n","    results_df['anomaly'] = ['Anomaly' if is_anomaly else 'Normal' for is_anomaly in anomalies]\n","\n","    return results_df\n","\n","def evaluate_model(y_true, y_pred):\n","    # Print classification report\n","    print(\"Classification Report:\")\n","    print(classification_report(y_true, y_pred, target_names=['Normal', 'Anomaly']))\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title('Confusion Matrix')\n","    plt.show()\n","\n","    # Plot ROC curve\n","    fpr, tpr, _ = roc_curve(y_true, y_pred)\n","    auc = roc_auc_score(y_true, y_pred)\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr, tpr, marker='o', label=f'ROC curve (AUC = {auc:.2f})')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('ROC Curve')\n","    plt.legend()\n","    plt.show()\n","\n","# Prepare features and labels\n","X = preprocess_samples(dataset_df, scaler)\n","y = dataset_df['label'].apply(lambda x: 1 if x == 'Anomaly' else 0)  # Assuming 'label' is the column for true labels\n","\n","# Split the dataset into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# Make predictions on the validation set\n","results_df = predict_anomalies(pd.DataFrame(X_val, columns=dataset_df.columns), scaler, autoencoder, threshold)\n","\n","# Evaluate model\n","evaluate_model(y_val, results_df['anomaly'].apply(lambda x: 1 if x == 'Anomaly' else 0))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"F13ZAIu9Qs1r","executionInfo":{"status":"error","timestamp":1720580324894,"user_tz":-180,"elapsed":1268,"user":{"displayName":"Azizah alqahtani","userId":"11981846926504147280"}},"outputId":"4952a0fb-cad0-4427-a108-4c8fbc73caf1"},"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["Columns in the dataset: Index(['date_time', 'duration', 'source_ip', 'destination_ip', 'source_port',\n","       'destination_port', 'protocol', 'flag', 'forwarding_status', 'tos',\n","       'packets', 'bytes', 'label'],\n","      dtype='object')\n"]},{"output_type":"error","ename":"ValueError","evalue":"The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- label\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-101-52fb72328d3d>\u001b[0m in \u001b[0;36m<cell line: 135>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;31m# Prepare features and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Anomaly'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming 'label' is the column for true labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-101-52fb72328d3d>\u001b[0m in \u001b[0;36mpreprocess_samples\u001b[0;34m(samples_df, scaler)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Normalize the data using StandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0msamples_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_df\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use the pre-fitted scaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msamples_normalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    993\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mvalidated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 )\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     def _validate_data(\n","\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- label\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import struct\n","import socket\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tensorflow.keras.models import load_model\n","from joblib import load\n","\n","# Define file paths\n","scaler_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/scaler.joblib'\n","autoencoder_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/autoencoder_model.keras'\n","threshold_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/threshold.npy'\n","dataset_file_path = '/content/drive/MyDrive/azizah_alqahtani_project/IDS_codes/new_dataset.csv'\n","\n","# Load resources\n","scaler = load(scaler_file_path)\n","autoencoder = load_model(autoencoder_file_path)\n","threshold = np.load(threshold_file_path)\n","\n","# Load the dataset\n","dataset_df = pd.read_csv(dataset_file_path)\n","\n","# Print columns to check for discrepancies\n","print(\"Columns in the dataset:\", dataset_df.columns)\n","\n","# Categorical features to be encoded\n","categorical_features = ['protocol', 'flag']\n","\n","# Initialize mappings dictionary\n","mappings = {}\n","\n","# Perform label encoding for categorical features and store mappings\n","for feature in categorical_features:\n","    label_encoder = LabelEncoder()\n","    dataset_df[feature] = label_encoder.fit_transform(dataset_df[feature])\n","    mappings[feature] = {label: index for index, label in enumerate(label_encoder.classes_)}\n","\n","# Extract mappings for protocol and flag\n","protocol_mapping = mappings.get('protocol', {})\n","flag_mapping = mappings.get('flag', {})\n","\n","def ip_to_int(ip_address):\n","    try:\n","        return struct.unpack(\"!I\", socket.inet_aton(ip_address))[0]\n","    except socket.error:\n","        return 0  # Return 0 if IP address is invalid\n","\n","def preprocess_samples(samples_df, scaler):\n","    # Define feature columns (excluding 'label')\n","    feature_columns = ['duration', 'source_ip', 'destination_ip', 'source_port',\n","                       'destination_port', 'forwarding_status', 'protocol',\n","                       'flag', 'tos', 'packets', 'bytes']\n","\n","    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n","    samples_df = samples_df.copy()\n","\n","    # Convert IP addresses to integers\n","    samples_df['source_ip'] = samples_df['source_ip'].apply(ip_to_int)\n","    samples_df['destination_ip'] = samples_df['destination_ip'].apply(ip_to_int)\n","\n","    # Encode categorical features\n","    samples_df['protocol'] = samples_df['protocol'].map(protocol_mapping).fillna(-1).astype(int)\n","    samples_df['flag'] = samples_df['flag'].map(flag_mapping).fillna(-1).astype(int)\n","\n","    # Ensure all feature columns are present\n","    for feature in feature_columns:\n","        if feature not in samples_df:\n","            samples_df[feature] = 0  # Impute missing features with zeros\n","\n","    # Reorder columns to match the scaler's expected feature order\n","    samples_df = samples_df[feature_columns]\n","\n","    # Fill missing values in numerical features\n","    samples_df.fillna(0, inplace=True)\n","\n","    # Normalize the data using StandardScaler\n","    samples_normalized = scaler.transform(samples_df)\n","\n","    return samples_normalized\n","\n","def predict_anomalies(samples_df, scaler, autoencoder, threshold):\n","    # Preprocess the samples\n","    preprocessed_samples_df = preprocess_samples(samples_df, scaler)\n","\n","    # Predict reconstructed data using the autoencoder\n","    reconstructed_data = autoencoder.predict(preprocessed_samples_df)\n","\n","    # Calculate reconstruction errors\n","    reconstruction_errors = np.mean(np.square(preprocessed_samples_df - reconstructed_data), axis=1)\n","\n","    # Determine if each reconstruction error exceeds the threshold\n","    anomalies = reconstruction_errors > threshold\n","\n","    # Return a DataFrame with the results\n","    results_df = samples_df.copy()\n","    results_df['reconstruction_error'] = reconstruction_errors\n","    results_df['anomaly'] = ['Anomaly' if is_anomaly else 'Normal' for is_anomaly in anomalies]\n","\n","    return results_df\n","\n","def evaluate_model(y_true, y_pred):\n","    # Print classification report\n","    print(\"Classification Report:\")\n","    print(classification_report(y_true, y_pred, target_names=['Normal', 'Anomaly']))\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title('Confusion Matrix')\n","    plt.show()\n","\n","    # Plot ROC curve\n","    fpr, tpr, _ = roc_curve(y_true, y_pred)\n","    auc = roc_auc_score(y_true, y_pred)\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr, tpr, marker='o', label=f'ROC curve (AUC = {auc:.2f})')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('ROC Curve')\n","    plt.legend()\n","    plt.show()\n","\n","# Prepare features and labels\n","X = preprocess_samples(dataset_df, scaler)\n","y = dataset_df['label'].apply(lambda x: 1 if x == 'Anomaly' else 0)  # Assuming 'label' is the column for true labels\n","\n","# Split the dataset into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# Make predictions on the validation set\n","X_val_df = pd.DataFrame(X_val, columns=['duration', 'source_ip', 'destination_ip', 'source_port',\n","                                         'destination_port', 'forwarding_status', 'protocol',\n","                                         'flag', 'tos', 'packets', 'bytes'])\n","results_df = predict_anomalies(X_val_df, scaler, autoencoder, threshold)\n","\n","# Evaluate model\n","evaluate_model(y_val, results_df['anomaly'].apply(lambda x: 1 if x == 'Anomaly' else 0))\n"],"metadata":{"id":"wLK6MVLVlYNv","executionInfo":{"status":"error","timestamp":1720580412482,"user_tz":-180,"elapsed":921,"user":{"displayName":"Azizah alqahtani","userId":"11981846926504147280"}},"outputId":"ccfc4b88-4c6d-4857-94b7-a3e92c29c540","colab":{"base_uri":"https://localhost:8080/","height":443}},"execution_count":102,"outputs":[{"output_type":"stream","name":"stdout","text":["Columns in the dataset: Index(['date_time', 'duration', 'source_ip', 'destination_ip', 'source_port',\n","       'destination_port', 'protocol', 'flag', 'forwarding_status', 'tos',\n","       'packets', 'bytes', 'label'],\n","      dtype='object')\n"]},{"output_type":"error","ename":"ValueError","evalue":"The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-102-e1653a6b03e1>\u001b[0m in \u001b[0;36m<cell line: 132>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m# Prepare features and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Anomaly'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming 'label' is the column for true labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-102-e1653a6b03e1>\u001b[0m in \u001b[0;36mpreprocess_samples\u001b[0;34m(samples_df, scaler)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Normalize the data using StandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0msamples_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msamples_normalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    993\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mvalidated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 )\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     def _validate_data(\n","\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n"]}]},{"cell_type":"markdown","source":["#2. Adjusting the Model Architecture\n","Typically, you would modify the model for the new task, like adding or modifying layers. However, in the case of an autoencoder, I don't change the architecture much but focus on retraining it."],"metadata":{"id":"no0yX_jjm0ZL"}},{"cell_type":"markdown","source":["# 3. Retraining the Model\n"],"metadata":{"id":"ZanTiM9Eol7_"}},{"cell_type":"markdown","source":["# 4. Evaluating the Model"],"metadata":{"id":"_ZnHfCyxoyau"}},{"cell_type":"code","source":[],"metadata":{"id":"In0HYkOYo-6g","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"error","timestamp":1720575217905,"user_tz":-180,"elapsed":338,"user":{"displayName":"Azizah alqahtani","userId":"11981846926504147280"}},"outputId":"c25618d8-5b16-4a83-f72d-c5b37115ecbb"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-54-9f0797c2ab85>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Prepare features and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Anomaly'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming 'label' is the column for true labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-52-9ee017ef2f3e>\u001b[0m in \u001b[0;36mpreprocess_samples\u001b[0;34m(samples_df, scaler)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Normalize the data using StandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0msamples_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msamples_normalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    993\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mvalidated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 )\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     def _validate_data(\n","\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n"]}]},{"cell_type":"markdown","source":["# Next Steps:\n","# Save the Fine-Tuned Model:\n","After training, save the fine-tuned model for future use.\n","#Hyperparameter Tuning:\n","Adjust parameters like learning rate, number of epochs, or batch size to optimize performance.\n","# Model Comparison:\n","Compare the fine-tuned model's performance with a model trained from scratch to evaluate the benefits of transfer learning."],"metadata":{"id":"9CRkdNaZp2ea"}}]}